###############################################################
# Generate Mock Data for Blind Box SEM Study
# Matches demographic profile from original paper (Table 1)
###############################################################

import pandas as pd
import numpy as np
from scipy import stats as scipy_stats

# Set seed for reproducibility
np.random.seed(42)

# Sample size from original study
n = 375

###############################################################
# DEMOGRAPHIC VARIABLES (Control Variables)
###############################################################

# Gender: More females (based on paper - appears female-dominated)
# Paper doesn't give exact percentages, but blind box market is ~70% female
gender = np.random.choice(
    ['Male', 'Female'], 
    size=n, 
    p=[0.30, 0.70]
)

# Age: Mostly 18-25 years old (young consumers)
# Distribution based on Table 1:
age_categories = ['Under 18', '18-25', '26-30', '31-35', 'Over 35']
age_probs = [0.05, 0.55, 0.25, 0.10, 0.05]  # Heavy concentration in 18-25
age = np.random.choice(age_categories, size=n, p=age_probs)

# Education: Mainly undergraduate and junior college
# Based on Table 1 showing mostly college students
education_categories = ['High school or below', 'Junior college', 'Undergraduate', 'Graduate or above']
education_probs = [0.10, 0.30, 0.45, 0.15]
education = np.random.choice(education_categories, size=n, p=education_probs)

# Monthly Income: 46.9% over 5,000 yuan (from paper)
# Approximately: <2000, 2000-3000, 3000-5000, >5000
income_categories = ['Under 2000', '2000-3000', '3000-5000', 'Over 5000']
income_probs = [0.15, 0.18, 0.20, 0.47]  # 46.9% over 5000
monthly_income = np.random.choice(income_categories, size=n, p=income_probs)

# Purchase frequency (additional control variable - common in consumer studies)
purchase_freq_categories = ['First time', 'Occasionally', 'Regularly', 'Frequently']
purchase_frequency = np.random.choice(
    purchase_freq_categories, 
    size=n, 
    p=[0.15, 0.35, 0.35, 0.15]
)

###############################################################
# CONSUMPTION PURPOSE (Moderator Variable)
###############################################################

# From paper: N=208 process-oriented, N=167 result-oriented
# That's 55.5% process, 44.5% result
purpose = np.random.choice(
    [0, 1],  # 0=result-oriented, 1=process-oriented
    size=n,
    p=[0.445, 0.555]
)

###############################################################
# LATENT VARIABLE INDICATORS
###############################################################

# We'll generate correlated data that reflects the paper's findings:
# - Uncertainty → Emotional Value: POSITIVE (β = 0.443)
# - Uncertainty → Social Value: POSITIVE (β = 0.339)
# - Emotional Value → Purchase Intention: POSITIVE (β = 0.392)
# - Social Value → Purchase Intention: POSITIVE (β = 0.290)

# Create base latent scores
uncertainty_base = np.random.normal(0, 1, n)
emotional_value_base = 0.443 * uncertainty_base + np.random.normal(0, 0.7, n)
social_value_base = 0.339 * uncertainty_base + np.random.normal(0, 0.8, n)
purchase_intention_base = (
    0.392 * emotional_value_base + 
    0.290 * social_value_base + 
    np.random.normal(0, 0.5, n)
)

# Add moderator effects
# Process-oriented consumers have stronger emotional responses
purpose_effect_ev = np.where(purpose == 1, 0.15, 0)  # Boost for process-oriented
emotional_value_base += purpose_effect_ev * uncertainty_base

# Result-oriented consumers focus more on outcomes
purpose_effect_pi = np.where(purpose == 0, 0.10, 0)
purchase_intention_base += purpose_effect_pi

# Standardize to 0-1 range then scale to 1-7 (Likert scale)
def standardize_to_likert(arr, mean=4, std=1.2):
    """Convert normal distribution to 7-point Likert scale"""
    standardized = (arr - arr.mean()) / arr.std()
    scaled = mean + (standardized * std)
    # Clip to 1-7 range and round to nearest 0.5 (common in surveys)
    clipped = np.clip(scaled, 1, 7)
    return np.round(clipped * 2) / 2  # Round to nearest 0.5

# Generate individual items with factor loadings
# Uncertainty items (4 items)
unc1 = standardize_to_likert(uncertainty_base + np.random.normal(0, 0.3, n))
unc2 = standardize_to_likert(uncertainty_base + np.random.normal(0, 0.3, n))
unc3 = standardize_to_likert(uncertainty_base + np.random.normal(0, 0.3, n))
unc4 = standardize_to_likert(uncertainty_base + np.random.normal(0, 0.3, n))

# Emotional Value items (3 items)
ev1 = standardize_to_likert(emotional_value_base + np.random.normal(0, 0.3, n))
ev2 = standardize_to_likert(emotional_value_base + np.random.normal(0, 0.3, n))
ev3 = standardize_to_likert(emotional_value_base + np.random.normal(0, 0.3, n))

# Social Value items (3 items)
sv1 = standardize_to_likert(social_value_base + np.random.normal(0, 0.3, n))
sv2 = standardize_to_likert(social_value_base + np.random.normal(0, 0.3, n))
sv3 = standardize_to_likert(social_value_base + np.random.normal(0, 0.3, n))

# Purchase Intention items (3 items)
pi1 = standardize_to_likert(purchase_intention_base + np.random.normal(0, 0.3, n))
pi2 = standardize_to_likert(purchase_intention_base + np.random.normal(0, 0.3, n))
pi3 = standardize_to_likert(purchase_intention_base + np.random.normal(0, 0.3, n))

###############################################################
# CREATE DATAFRAME
###############################################################

df = pd.DataFrame({
    # Demographics (Control Variables)
    'gender': gender,
    'age': age,
    'education': education,
    'monthly_income': monthly_income,
    'purchase_frequency': purchase_frequency,
    
    # Moderator Variable
    'purpose': purpose,
    
    # Uncertainty indicators
    'unc1': unc1,
    'unc2': unc2,
    'unc3': unc3,
    'unc4': unc4,
    
    # Emotional Value indicators
    'ev1': ev1,
    'ev2': ev2,
    'ev3': ev3,
    
    # Social Value indicators
    'sv1': sv1,
    'sv2': sv2,
    'sv3': sv3,
    
    # Purchase Intention indicators
    'pi1': pi1,
    'pi2': pi2,
    'pi3': pi3
})

###############################################################
# SAVE TO CSV
###############################################################

df.to_csv('mock_blindbox_sem_data.csv', index=False)

print("="*60)
print("MOCK DATA GENERATION COMPLETE")
print("="*60)
print(f"\nTotal observations: {len(df)}")

###############################################################
# DISPLAY DEMOGRAPHIC STATISTICS
###############################################################

print("\n" + "="*60)
print("DEMOGRAPHIC PROFILE (Control Variables)")
print("="*60)

print("\n1. GENDER DISTRIBUTION:")
print(df['gender'].value_counts())
print(f"\nPercentages:")
print(df['gender'].value_counts(normalize=True).round(3) * 100)

print("\n2. AGE DISTRIBUTION:")
print(df['age'].value_counts().sort_index())
print(f"\nPercentages:")
print(df['age'].value_counts(normalize=True).round(3) * 100)

print("\n3. EDUCATION LEVEL:")
print(df['education'].value_counts())
print(f"\nPercentages:")
print(df['education'].value_counts(normalize=True).round(3) * 100)

print("\n4. MONTHLY INCOME:")
print(df['monthly_income'].value_counts())
print(f"\nPercentages:")
print(df['monthly_income'].value_counts(normalize=True).round(3) * 100)

print("\n5. PURCHASE FREQUENCY:")
print(df['purchase_frequency'].value_counts())
print(f"\nPercentages:")
print(df['purchase_frequency'].value_counts(normalize=True).round(3) * 100)

print("\n6. CONSUMPTION PURPOSE (Moderator):")
purpose_labels = {0: 'Result-oriented', 1: 'Process-oriented'}
purpose_counts = df['purpose'].map(purpose_labels).value_counts()
print(purpose_counts)
print(f"\nPercentages:")
print(df['purpose'].map(purpose_labels).value_counts(normalize=True).round(3) * 100)

###############################################################
# DISPLAY DESCRIPTIVE STATISTICS FOR SCALE ITEMS
###############################################################

print("\n" + "="*60)
print("DESCRIPTIVE STATISTICS (Scale Items)")
print("="*60)

scale_items = [col for col in df.columns if col.startswith(('unc', 'ev', 'sv', 'pi'))]
desc_stats = df[scale_items].describe().round(2)
print("\n", desc_stats)

###############################################################
# CORRELATION MATRIX (Quick check)
###############################################################

print("\n" + "="*60)
print("CORRELATION MATRIX (Construct Averages)")
print("="*60)

df['Uncertainty_avg'] = df[['unc1', 'unc2', 'unc3', 'unc4']].mean(axis=1)
df['EmotionalValue_avg'] = df[['ev1', 'ev2', 'ev3']].mean(axis=1)
df['SocialValue_avg'] = df[['sv1', 'sv2', 'sv3']].mean(axis=1)
df['PurchaseIntention_avg'] = df[['pi1', 'pi2', 'pi3']].mean(axis=1)

constructs = ['Uncertainty_avg', 'EmotionalValue_avg', 'SocialValue_avg', 'PurchaseIntention_avg']
corr_matrix = df[constructs].corr().round(3)
print("\n", corr_matrix)

# Remove average columns before saving
df = df.drop(columns=[c for c in df.columns if c.endswith('_avg')])
df.to_csv('mock_blindbox_sem_data.csv', index=False)

print("\n" + "="*60)
print("DATA SAVED: mock_blindbox_sem_data.csv")
print("="*60)
print("\nThis dataset now includes:")
print("✓ Control variables (gender, age, education, income, purchase frequency)")
print("✓ Moderator variable (consumption purpose)")
print("✓ All scale items (uncertainty, emotional value, social value, purchase intention)")
print("✓ Demographic distribution matching original study")
print("✓ Correlations reflecting paper's findings")
